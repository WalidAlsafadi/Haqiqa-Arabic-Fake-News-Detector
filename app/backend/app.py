import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import json
import os

# Model configuration
MODEL_NAME = "aubmindlab/bert-base-arabertv02"
model_path = "WalidAlsafadi/arabert-fake-news-detector"  # Your Hugging Face model

class ArabicFakeNewsDetector:
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.load_model()
    
    def load_model(self):
        """Load the fine-tuned AraBERT model"""
        try:
            # Load tokenizer and model from Hugging Face Hub
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
            self.model = AutoModelForSequenceClassification.from_pretrained(
                model_path,
                num_labels=2
            )
            self.model.to(self.device)
            self.model.eval()
            print(f"Model loaded successfully on {self.device}")
        except Exception as e:
            print(f"Error loading model: {e}")
            # Fallback to base model for demo purposes
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
            self.model = AutoModelForSequenceClassification.from_pretrained(
                MODEL_NAME,
                num_labels=2
            )
            self.model.to(self.device)
            self.model.eval()
    
    def predict(self, text):
        """Predict if Arabic news text is real or fake"""
        if not text or text.strip() == "":
            return {"error": "Ø§Ù„Ù†Øµ ÙØ§Ø±Øº. ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ù„Ù„ØªØ­Ù„ÙŠÙ„."}
        
        try:
            # Tokenize input
            inputs = self.tokenizer(
                text,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="pt"
            )
            
            # Move to device
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # Get prediction
            with torch.no_grad():
                outputs = self.model(**inputs)
                probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
                
            # Convert to percentages
            fake_prob = float(probabilities[0][0]) * 100
            real_prob = float(probabilities[0][1]) * 100
            
            # Determine prediction
            prediction = "Ø­Ù‚ÙŠÙ‚ÙŠ" if real_prob > fake_prob else "Ù…Ø²ÙŠÙ"
            confidence = max(real_prob, fake_prob)
            
            return {
                "prediction": prediction,
                "confidence": round(confidence, 2),
                "Real": round(real_prob, 2),
                "Fake": round(fake_prob, 2)
            }
            
        except Exception as e:
            return {"error": f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)}"}

# Initialize detector
detector = ArabicFakeNewsDetector()

def predict_news(text):
    """Gradio interface function"""
    result = detector.predict(text)
    
    if "error" in result:
        return result["error"]
    
    # Format output for Gradio
    output = f"""
    **Ø§Ù„ØªÙ†Ø¨Ø¤:** {result['prediction']}
    **Ø§Ù„Ø«Ù‚Ø©:** {result['confidence']:.1f}%
    
    **Ø§Ù„ØªÙØ§ØµÙŠÙ„:**
    - Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø®Ø¨Ø± Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ: {result['Real']:.1f}%
    - Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø®Ø¨Ø± Ø§Ù„Ù…Ø²ÙŠÙ: {result['Fake']:.1f}%
    """
    
    return output

# Create Gradio interface
with gr.Blocks(
    title="ÙƒØ§Ø´Ù Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø²ÙŠÙØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© - AraBERT",
    theme=gr.themes.Soft(),
    css="""
    .gradio-container {
        direction: rtl;
        text-align: right;
    }
    .gr-textbox textarea {
        direction: rtl;
        text-align: right;
    }
    """
) as demo:
    
    gr.Markdown("""
    # ğŸ” ÙƒØ§Ø´Ù Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø²ÙŠÙØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    ### Ù…Ø¯Ø¹ÙˆÙ… Ø¨Ù†Ù…ÙˆØ°Ø¬ AraBERT Ø§Ù„Ù…ÙØ¯Ø±Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠØ©
    
    Ø£Ø¯Ø®Ù„ Ù†Øµ Ø§Ù„Ø®Ø¨Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙ†Ø¨Ø¤ Ø­ÙˆÙ„ ØµØ­ØªÙ‡
    """)
    
    with gr.Row():
        with gr.Column():
            text_input = gr.Textbox(
                label="Ù†Øµ Ø§Ù„Ø®Ø¨Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠ",
                placeholder="Ø£Ø¯Ø®Ù„ Ù†Øµ Ø§Ù„Ø®Ø¨Ø± Ù‡Ù†Ø§...",
                lines=5,
                rtl=True
            )
            
            predict_btn = gr.Button(
                "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¨Ø±",
                variant="primary"
            )
    
    with gr.Row():
        output = gr.Markdown(
            label="Ø§Ù„Ù†ØªÙŠØ¬Ø©",
            rtl=True
        )
    
    # Event handlers
    predict_btn.click(
        fn=predict_news,
        inputs=[text_input],
        outputs=[output]
    )
    
    text_input.submit(
        fn=predict_news,
        inputs=[text_input],
        outputs=[output]
    )
    
    # Examples
    gr.Examples(
        examples=[
            ["Ø£Ø¹Ù„Ù†Øª ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø© Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠØ© Ø¹Ù† ØªØ³Ø¬ÙŠÙ„ Ø­Ø§Ù„Ø§Øª Ø¥ØµØ§Ø¨Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø¨ÙÙŠØ±ÙˆØ³ ÙƒÙˆØ±ÙˆÙ†Ø§ ÙÙŠ Ù‚Ø·Ø§Ø¹ ØºØ²Ø©"],
            ["Ø´Ø±ÙƒØ© ØªÙ‚Ù†ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø© ØªØ¹Ù„Ù† Ø¹Ù† Ø§Ø®ØªØ±Ø§Ù‚ Ø¹Ù„Ù…ÙŠ Ù…Ø°Ù‡Ù„ Ø³ÙŠØºÙŠØ± Ø§Ù„Ø¹Ø§Ù„Ù… Ø®Ù„Ø§Ù„ Ø£Ø³Ø¨ÙˆØ¹"],
            ["Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© ÙÙŠ ØºØ²Ø© ØªØ³ØªÙ‚Ø¨Ù„ ÙˆÙØ¯Ø§Ù‹ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ§Ù‹ Ù…Ù† Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø£Ø²Ù‡Ø±"]
        ],
        inputs=[text_input],
        outputs=[output],
        fn=predict_news,
        cache_examples=False
    )

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )